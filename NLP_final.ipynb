{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a9ec94e",
   "metadata": {},
   "source": [
    "Read embeddings matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "908b35f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pride829\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "# Create embeddings_index\n",
    "\n",
    "kv = KeyedVectors.load_word2vec_format(\n",
    "        os.path.join('data', 'GoogleNews-vectors-negative300.bin'), \n",
    "        binary = True\n",
    "      )\n",
    "\n",
    "embeddings_index = {}\n",
    "for word, vector in zip(list(kv.index_to_key), kv.vectors):\n",
    "    coefs = np.asarray(vector, dtype='float32')\n",
    "    embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732d9ea7",
   "metadata": {},
   "source": [
    "Read pure data and split it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "822d26f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import string\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import one_hot\n",
    "\n",
    "tf.random.set_seed(1137)\n",
    "random.seed(1137)\n",
    "\n",
    "def preprocess(text):\n",
    "    # Split the text by space\n",
    "    \n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    text = text.split()\n",
    "    return text\n",
    "\n",
    "def to_embedding(tokens, embeddings_index):\n",
    "    # Transform the tokens into embeddings\n",
    "    \n",
    "    embeddings = []\n",
    "    for i in range(0, len(tokens)):\n",
    "        try:\n",
    "            embeddings.append(embeddings_index[tokens[i]])\n",
    "        except:\n",
    "            continue\n",
    "    return embeddings\n",
    "\n",
    "def add_padding(embeddings, padding_width = None):\n",
    "    #\n",
    "    \n",
    "    emb_padded = pad_sequences(embeddings, maxlen=padding_width, padding='pre', dtype='float32')\n",
    "    return emb_padded\n",
    "\n",
    "def process_text(sentences, embeddings_index, padding = None):\n",
    "    # Combine the processes\n",
    "    \n",
    "    result = [ preprocess(sentence) for sentence in sentences ]\n",
    "    result = [ to_embedding(sentence, embeddings_index) for sentence in result ]\n",
    "    result = add_padding(result, padding)\n",
    "    return result\n",
    "\n",
    "# Define senses\n",
    "SENSE = {\n",
    "    1: 'a social event at which a group of people meet to talk, eat, drink, dance, etc.', # 派對\n",
    "    2: 'an organization of people with particular political beliefs', # 政黨\n",
    "    3: 'a single entity which can be identified as one for the purposes of the law' # （法庭）當事人；⋯⋯方\n",
    "}\n",
    "\n",
    "# Read labeled data\n",
    "\n",
    "with open(os.path.join('data', 'party.labeled.txt'), 'r', encoding=\"utf-8\") as f:\n",
    "    data = f.read().strip().split('\\n')\n",
    "\n",
    "\n",
    "pure_data = [[text, label] for sent_id, label, text in [line.split('\\t', 2) for line in data]]\n",
    "\n",
    "# Split the pure_data\n",
    "\n",
    "middle = 50\n",
    "\n",
    "pure_data_train = pure_data[:middle]\n",
    "pure_data_test = pure_data[middle:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d82a652",
   "metadata": {},
   "source": [
    "Data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d08f10",
   "metadata": {},
   "source": [
    "Method 1: Random words replacing with similar word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "19617d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace some words in the original sentence\n",
    "\n",
    "def replace_random_word(pure_data_train):\n",
    "    NUMBER_REPLACED = 6\n",
    "\n",
    "    pdt_r = []\n",
    "\n",
    "    for d in pure_data_train:\n",
    "\n",
    "        text = d[0]\n",
    "        label = d[1]\n",
    "\n",
    "        # Clean the punctuation\n",
    "        translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "        text = text.translate(translator)\n",
    "\n",
    "        t = text.split()\n",
    "\n",
    "        pdt_r.append([' '.join(t), label])\n",
    "\n",
    "        # Generate the indices of the words that are about to being replaced\n",
    "        replaced_index = []\n",
    "\n",
    "        while(len(replaced_index) < NUMBER_REPLACED):\n",
    "            r = random.randrange(len(t))\n",
    "            if(t[r] == 'party' or r in replaced_index):\n",
    "                continue\n",
    "            replaced_index.append(r)\n",
    "\n",
    "        for idx in replaced_index:\n",
    "\n",
    "            temp_t = t.copy()\n",
    "            # Replace the word with the most similar word\n",
    "            try:\n",
    "                temp_t[idx] = kv.most_similar(positive=[t[idx]], topn=1)[0][0]\n",
    "                pdt_r.append([' '.join(temp_t), label])\n",
    "            except:\n",
    "                pass\n",
    "    return pdt_r\n",
    "\n",
    "pdt_r = replace_random_word(pure_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ab52de56",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pdt_r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcce5e1",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "36058a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train size: Test size]: [308: 657]\n",
      "Epoch 1/5\n",
      "154/154 [==============================] - 2s 5ms/step - loss: 0.8005 - accuracy: 0.6104\n",
      "Epoch 2/5\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.2962 - accuracy: 0.9318\n",
      "Epoch 3/5\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.0539 - accuracy: 0.9933 ETA: 0s - loss: 0.0583 - accuracy:  - 1s 5ms/step - loss: 0.0528 - accuracy: 0.9935\n",
      "Epoch 4/5\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.0060 - accuracy: 1.0000\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.8439 - accuracy: 0.7580\n",
      "Test loss: 0.8439046740531921\n",
      "Test accuracy: 0.7579908967018127\n"
     ]
    }
   ],
   "source": [
    "# Process input\n",
    "X_train = process_text([d[0] for d in data_train], embeddings_index)\n",
    "X_test = process_text([d[0] for d in pure_data_test], embeddings_index)\n",
    "\n",
    "Y_train = [int(d[1])-1 for d in data_train]\n",
    "Y_train = tf.one_hot(Y_train, 3, axis=1, dtype=tf.float32)\n",
    "Y_test = [int(d[1])-1 for d in pure_data_test]\n",
    "Y_test = tf.one_hot(Y_test, 3, axis=1, dtype=tf.float32)\n",
    "\n",
    "print(f\"[Train size: Test size]: [{X_train.shape[0]}: {X_test.shape[0]}]\")\n",
    "\n",
    "# Training\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "_, PADDING_WIDTH, EMBEDDING_DIM = X_train.shape\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 5\n",
    "OUTPUT_CATEGORY = len(SENSE)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(tf.keras.layers.LSTM(4))\n",
    "model.add(tf.keras.layers.Dense(64))\n",
    "model.add(tf.keras.layers.Dense(OUTPUT_CATEGORY, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='Adam',\n",
    "                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "\n",
    "results = model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE)\n",
    "print(f\"Test loss: {results[0]}\")\n",
    "print(f\"Test accuracy: {results[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa729460",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
