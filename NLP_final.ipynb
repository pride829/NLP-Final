{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a9ec94e",
   "metadata": {},
   "source": [
    "Read embeddings matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "908b35f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "# Create embeddings_index\n",
    "\n",
    "kv = KeyedVectors.load_word2vec_format(\n",
    "        os.path.join('data', 'GoogleNews-vectors-negative300.bin'), \n",
    "        binary = True\n",
    "      )\n",
    "\n",
    "embeddings_index = {}\n",
    "for word, vector in zip(list(kv.index_to_key), kv.vectors):\n",
    "    coefs = np.asarray(vector, dtype='float32')\n",
    "    embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732d9ea7",
   "metadata": {},
   "source": [
    "Read pure data and split it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "822d26f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import string\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import one_hot\n",
    "\n",
    "tf.random.set_seed(1337)\n",
    "random.seed(1337)\n",
    "\n",
    "def preprocess(text):\n",
    "    # Split the text by space\n",
    "    \n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    text = text.split()\n",
    "    return text\n",
    "\n",
    "def to_embedding(tokens, embeddings_index):\n",
    "    # Transform the tokens into embeddings\n",
    "    \n",
    "    embeddings = []\n",
    "    for i in range(0, len(tokens)):\n",
    "        try:\n",
    "            embeddings.append(embeddings_index[tokens[i]])\n",
    "        except:\n",
    "            continue\n",
    "    return embeddings\n",
    "\n",
    "def add_padding(embeddings, padding_width = None):\n",
    "    #\n",
    "    \n",
    "    emb_padded = pad_sequences(embeddings, maxlen=padding_width, padding='pre', dtype='float32')\n",
    "    return emb_padded\n",
    "\n",
    "def process_text(sentences, embeddings_index, padding = None):\n",
    "    # Combine the processes\n",
    "    \n",
    "    result = [ preprocess(sentence) for sentence in sentences ]\n",
    "    result = [ to_embedding(sentence, embeddings_index) for sentence in result ]\n",
    "    result = add_padding(result, padding)\n",
    "    return result\n",
    "\n",
    "# Define senses\n",
    "SENSE = {\n",
    "    1: 'a social event at which a group of people meet to talk, eat, drink, dance, etc.', # 派對\n",
    "    2: 'an organization of people with particular political beliefs', # 政黨\n",
    "    3: 'a single entity which can be identified as one for the purposes of the law' # （法庭）當事人；⋯⋯方\n",
    "}\n",
    "\n",
    "# Read labeled data\n",
    "\n",
    "with open(os.path.join('data', 'party.labeled.txt'), 'r', encoding=\"utf-8\") as f:\n",
    "    data = f.read().strip().split('\\n')\n",
    "\n",
    "\n",
    "pure_data = [[text, label] for sent_id, label, text in [line.split('\\t', 2) for line in data]]\n",
    "\n",
    "# Split the pure_data\n",
    "\n",
    "middle = 100\n",
    "\n",
    "pure_data_train = pure_data[:middle]\n",
    "pure_data_test = pure_data[middle:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d82a652",
   "metadata": {},
   "source": [
    "Data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d08f10",
   "metadata": {},
   "source": [
    "Method 1: Random words replacing with similar word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19617d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace some words in the original sentence\n",
    "\n",
    "def replace_random_word(pure_data_train):\n",
    "    NUMBER_REPLACED = 4\n",
    "\n",
    "    pdt_r = []\n",
    "\n",
    "    for d in pure_data_train:\n",
    "\n",
    "        text = d[0]\n",
    "        label = d[1]\n",
    "\n",
    "        # Clean the punctuation\n",
    "        translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "        text = text.translate(translator)\n",
    "        t = text.split()\n",
    "\n",
    "        pdt_r.append([' '.join(t), label])\n",
    "\n",
    "        # Generate the indices of the words that are about to being replaced\n",
    "        replaced_index = []\n",
    "\n",
    "        while(len(replaced_index) < NUMBER_REPLACED):\n",
    "            r = random.randrange(len(t))\n",
    "            if(t[r] == 'party' or r in replaced_index):\n",
    "                continue\n",
    "            replaced_index.append(r)\n",
    "\n",
    "        for idx in replaced_index:\n",
    "\n",
    "            temp_t = t.copy()\n",
    "            # Replace the word with the most similar word\n",
    "            try:\n",
    "                temp_t[idx] = kv.most_similar(positive=[t[idx]], topn=1)[0][0]\n",
    "                pdt_r.append([' '.join(temp_t), label])\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    return pdt_r\n",
    "\n",
    "pdt_r = replace_random_word(pure_data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464c17a2",
   "metadata": {},
   "source": [
    "Method 2: Back Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb151edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import translate\n",
    "\n",
    "credential_path = os.path.join(os.path.expanduser('~'), 'translator.json')\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_path\n",
    "\n",
    "def translate_text(\n",
    "    text='Hello, world!', \n",
    "    project_id='directed-bongo-336812', \n",
    "    source_language_code='en-US', \n",
    "    target_language_code='zh-TW'):\n",
    "\n",
    "    client = translate.TranslationServiceClient()\n",
    "    location = \"global\"\n",
    "    parent = f\"projects/{project_id}/locations/{location}\"\n",
    "\n",
    "    response = client.translate_text(\n",
    "        request={\n",
    "            \"parent\": parent,\n",
    "            \"contents\": [text],\n",
    "            \"mime_type\": \"text/plain\",\n",
    "            \"source_language_code\": source_language_code,\n",
    "            \"target_language_code\": target_language_code,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return response.translations[0].translated_text\n",
    "\n",
    "def back_translation(pure_data_train):\n",
    "    \n",
    "    \n",
    "    project_id = 'directed-bongo-336812'\n",
    "    \n",
    "    pdt_b = []\n",
    "    \n",
    "    for d in pure_data_train:\n",
    "        \n",
    "        pdt_b.append(d)\n",
    "        \n",
    "        text = d[0]\n",
    "        label = d[1]\n",
    "        \n",
    "        tw_text = translate_text(\n",
    "            text=text, \n",
    "            project_id=project_id, \n",
    "            source_language_code='en-US', \n",
    "            target_language_code='zh-TW')\n",
    "        \n",
    "        back_tw_text = translate_text(\n",
    "            text=tw_text, \n",
    "            project_id=project_id, \n",
    "            source_language_code='zh-TW', \n",
    "            target_language_code='en-US')\n",
    "        \n",
    "        pdt_b.append([back_tw_text, label])\n",
    "        \n",
    "        pl_text = translate_text(\n",
    "            text=text, \n",
    "            project_id=project_id, \n",
    "            source_language_code='en-US', \n",
    "            target_language_code='pl')\n",
    "        \n",
    "        back_pl_text = translate_text(\n",
    "            text=pl_text, \n",
    "            project_id=project_id, \n",
    "            source_language_code='pl', \n",
    "            target_language_code='en-US')\n",
    "        \n",
    "        pdt_b.append([back_pl_text, label])\n",
    "        \n",
    "        th_text = translate_text(\n",
    "            text=text, \n",
    "            project_id=project_id, \n",
    "            source_language_code='en-US', \n",
    "            target_language_code='th')\n",
    "        \n",
    "        back_th_text = translate_text(\n",
    "            text=th_text, \n",
    "            project_id=project_id, \n",
    "            source_language_code='th', \n",
    "            target_language_code='en-US')\n",
    "        \n",
    "        pdt_b.append([back_th_text, label])\n",
    "        \n",
    "    return pdt_b\n",
    "    \n",
    "pdt_b = back_translation(pure_data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da376b4",
   "metadata": {},
   "source": [
    "Method 3: Delete random word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ab580b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_random_word(pure_data_train):\n",
    "    pdt_d = []\n",
    "    \n",
    "    for d in pure_data_train:\n",
    "        \n",
    "        pdt_d.append(d)\n",
    "        \n",
    "        text = d[0]\n",
    "        label = d[1]\n",
    "        \n",
    "        # Clean the punctuation\n",
    "        translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "        text = text.translate(translator)\n",
    "        t = text.split()\n",
    "        \n",
    "        r0 = random.randrange(len(t))\n",
    "        r1 = random.randrange(len(t))\n",
    "        while(t[r0] == 'party' or t[r1] == 'party' or r0 == r1):\n",
    "            r0 = random.randrange(len(t))\n",
    "            r1 = random.randrange(len(t))\n",
    "        \n",
    "        temp_t = t.copy()\n",
    "        del temp_t[r0]\n",
    "        pdt_d.append([' '.join(temp_t), label])\n",
    "        \n",
    "        temp_t = t.copy()\n",
    "        del temp_t[r1]\n",
    "        pdt_d.append([' '.join(temp_t), label])\n",
    "        \n",
    "        temp_t = t.copy()\n",
    "        del temp_t[r0]\n",
    "        del temp_t[r1-1]\n",
    "        pdt_d.append([' '.join(temp_t), label])\n",
    "        \n",
    "    return pdt_d\n",
    "        \n",
    "pdt_d = delete_random_word(pure_data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ea0b8b",
   "metadata": {},
   "source": [
    "Method 4: Swap random two word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dee43ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_random_words(pure_data_train):\n",
    "    pdt_s = []\n",
    "    \n",
    "    for d in pure_data_train:\n",
    "        \n",
    "        pdt_s.append(d)\n",
    "        \n",
    "        text = d[0]\n",
    "        label = d[1]\n",
    "        \n",
    "        # Clean the punctuation\n",
    "        translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "        text = text.translate(translator)\n",
    "        t = text.split()\n",
    "        \n",
    "        r0 = random.randrange(len(t))\n",
    "        r1 = random.randrange(len(t))\n",
    "        while(t[r0] == 'party' or t[r1] == 'party' or r0 == r1):\n",
    "            r0 = random.randrange(len(t))\n",
    "            r1 = random.randrange(len(t))\n",
    "        \n",
    "        t = t.copy()\n",
    "        \n",
    "        temp = t[r0]\n",
    "        t[r0] = t[r1]\n",
    "        t[r1] = temp\n",
    "\n",
    "        pdt_s.append([' '.join(t), label])\n",
    "        \n",
    "        \n",
    "        r0 = random.randrange(len(t))\n",
    "        r1 = random.randrange(len(t))\n",
    "        while(t[r0] == 'party' or t[r1] == 'party' or r0 == r1):\n",
    "            r0 = random.randrange(len(t))\n",
    "            r1 = random.randrange(len(t))\n",
    "            \n",
    "        temp = t[r0]\n",
    "        t[r0] = t[r1]\n",
    "        t[r1] = temp\n",
    "        \n",
    "        pdt_s.append([' '.join(t), label])\n",
    "        \n",
    "        r0 = random.randrange(len(t))\n",
    "        r1 = random.randrange(len(t))\n",
    "        while(t[r0] == 'party' or t[r1] == 'party' or r0 == r1):\n",
    "            r0 = random.randrange(len(t))\n",
    "            r1 = random.randrange(len(t))\n",
    "            \n",
    "        temp = t[r0]\n",
    "        t[r0] = t[r1]\n",
    "        t[r1] = temp\n",
    "        \n",
    "        pdt_s.append([' '.join(t), label])\n",
    "        \n",
    "    return pdt_s\n",
    "        \n",
    "pdt_s = swap_random_words(pure_data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3757a534",
   "metadata": {},
   "source": [
    "Combine the augmentation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83e7d6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_train = pure_data_train.copy()\n",
    "\n",
    "final_data_train = swap_random_words(final_data_train)\n",
    "final_data_train = delete_random_word(final_data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcce5e1",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36058a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train size: Test size]: [1600: 607]\n",
      "Epoch 1/3\n",
      "800/800 [==============================] - 5s 6ms/step - loss: 0.2741 - accuracy: 0.8925\n",
      "Epoch 2/3\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0052 - accuracy: 0.9994\n",
      "Epoch 3/3\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.0054 - accuracy: 0.9987\n",
      "304/304 [==============================] - 1s 3ms/step - loss: 0.7085 - accuracy: 0.8303\n",
      "Test loss: 0.7085275650024414\n",
      "Test accuracy: 0.8303130269050598\n"
     ]
    }
   ],
   "source": [
    "# Specify the data source\n",
    "data_train = final_data_train\n",
    "\n",
    "# Process input\n",
    "X_train = process_text([d[0] for d in data_train], embeddings_index)\n",
    "X_test = process_text([d[0] for d in pure_data_test], embeddings_index)\n",
    "\n",
    "Y_train = [int(d[1])-1 for d in data_train]\n",
    "Y_train = tf.one_hot(Y_train, 3, axis=1, dtype=tf.float32)\n",
    "Y_test = [int(d[1])-1 for d in pure_data_test]\n",
    "Y_test = tf.one_hot(Y_test, 3, axis=1, dtype=tf.float32)\n",
    "\n",
    "print(f\"[Train size: Test size]: [{X_train.shape[0]}: {X_test.shape[0]}]\")\n",
    "\n",
    "# Training\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "_, PADDING_WIDTH, EMBEDDING_DIM = X_train.shape\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 3\n",
    "OUTPUT_CATEGORY = len(SENSE)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(tf.keras.layers.LSTM(4))\n",
    "model.add(tf.keras.layers.Dense(64))\n",
    "model.add(tf.keras.layers.Dense(OUTPUT_CATEGORY, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='Adam',\n",
    "                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "\n",
    "results = model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE)\n",
    "print(f\"Test loss: {results[0]}\")\n",
    "print(f\"Test accuracy: {results[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa729460",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
